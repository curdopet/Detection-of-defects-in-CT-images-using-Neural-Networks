# Detection of defects in CT images using Neural Networks

This repository contains a bachelor thesis (and its source codes) that aims to research and implement a possible solution of the [RSNA Intracranial Hemorrhage competition](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection "RSNA Intracranial Hemorrhage Detection | Kaggle").

The repository structure is following:
```
|
+ src                   directory with source codes
+ thesis                directory with the thesis and LaTeX code
   + tex                directory with LaTeX source codes of the thesis
```

The weights of the final models and the dataset used for experiments can be found on [Google Drive](https://drive.google.com/drive/folders/1SW-XUfy8fM4bJQSKNgBAjlKbxqj0-Coq?usp=sharing "Detection of defects in CT images using Neural Networks | Google Drive").

## Data
The data aren't provided in this repository, because the redistribution or publishing the competition data to anyone not participaring in the competition is prohibited. They can be downloaded from the [RSNA Intracranial Hemorrhage competition site](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection "RSNA Intracranial Hemorrhage Detection | Kaggle") on Kaggle. But you must be registered on Kaggle and agree with the competition rules to be able to download the data.

## Running the code
All the code is in the src directory. These notebooks used to be run on Kaggle and Google Colab, but the paths are configured to the Google Colab environment, so if you want to use it on Kaggle, you must modify the paths in the source code. Also, downloadind the data to Google Drive and mounting the drive to the Google Colab is recommended.

The directory in Google Colab must have the following structure:
```
|
+ drive                                            mounted Google Drive directory
|  + MyDrive
|     + rsna-intracranial-hemorrhage-detection     directory downloaded from the Kaggle competition
|     |  + stage_2_train                           directory with the DICOM training data
|     |  + stage_2_test                            directory with the DICOM test data
+ jpg_data                                         directory for the data jpg generated by the notebooks
|  + stage_2_train_jpg                             directory for the jpg training data
|  + stage_2_test_jpg                              directory for the jpg test data
+ dataframes                                       directory for the custom dataframes generated by the notebooks
+ weights                                          directory with the pre-trained weights
+ predictions                                      directory for the final predictions
+ src                                              directory with the source notebooks
```
Also, note that some required packages have to be installed in the Google Colab environment. The packages that need to be installed are ```pydicom``` and ```wandb```.
### Preparing the dataframes
The first thing which has to be done is to prepare the dataframes. For running the notebooks is necessary to add more information into the dataframes and modify them. It can be done by running the dataframe_preprocessing.ipynb in the src directory. If obtaining the dicom images from drive is too slow, is possible to run this code of Kaggle, but the paths in the notebook must be modified.
### Preparing the data
Because preprocessing the images during the training is extremely time-consuming, it is recommended to preprocess all data before the training and convert it to JPEG format to save some space. The notebook for the conversion is convert_data_to_jpg.ipynb in the src directory. Note that the conversion also takes time, so it is possible to run it multiple in parallel. And after modifying the paths, is also possible to run the code on Kaggle to obtain the data faster.
### Experiments
The experiments can be run by the experiments.ipynb in the src directory. If you want, you can modify the experiment settings. Also, notice that the experiments' results are plotted by wandb.ai, so you probably have to create an account there.
The experiments can be run with both JPEG and DICOM data, but using the JPEG data is recommended.

**Note** - Links to the performed experiments:

* [preprocessing experiments](https://wandb.ai/curdopet/Detection-of-defects-in-CT-images/reports/Image-Preprocessing-Dashboard--Vmlldzo2Njg4MDY "Image Preprocessing Dashboard | Weights & Biases")
* [model experiments](https://wandb.ai/curdopet/Detection-of-defects-in-CT-images/reports/Model-Experiments-Dashboard--Vmlldzo2NzEyMTM "Model Experiments Dashboard | Weights & Biases")

### Training the final solution and evaluation
The final solution is trained in the final_model.ipynb in the src directory. In this notebook, the models can also be evaluated on the test data, and the submissions dataframe for the competition can be generated.
