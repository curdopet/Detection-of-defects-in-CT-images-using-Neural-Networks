{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import pydicom\n","import pandas as pd\n","from tqdm.notebook import tqdm"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run notebooks with functions necessary for this notebook\n","# please modify the path if it differs\n","%run /content/src/image_preprocessing.ipynb"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def img_preprocessing(image, b_neigh=None, t_neigh=None, dim=(224,224), window=(40,80), apply_clahe=False):\n","    \"\"\"\n","    Preprocess an CT image using correcting HU values, brain segmentation, new spacing application, reshaping and converting to three channel image.\n","        \n","    :param image: the image to preprocess\n","    :param b_neigh: the bottom neighbouring slice\n","    :param t_neigh: the top neighbouring slice\n","    :param dim: a tuple of the desired dimensions of the image\n","    :param window: a tuple representing the window center and window width\n","    :param apply_clahe: whether to use CLAHE or not\n","        \n","    :return: the preprocessed image\n","    \"\"\"\n","    if b_neigh is not None and t_neigh is not None:\n","        channels = []\n","        for img in (b_neigh, image, t_neigh):\n","            pixel_array = img.pixel_array\n","            pixel_array = hu_to_pixels(pixel_array, img.RescaleIntercept, img.RescaleSlope, window[0], window[1])\n","            pixel_array = segment_brain(pixel_array)\n","            pixel_array = apply_new_spacing(pixel_array, np.array(img.PixelSpacing), [1, 1])\n","            pixel_array = crop_or_reshape(pixel_array, dim)\n","            if apply_clahe:\n","                pixel_array = clahe(pixel_array)\n","            channels.append(pixel_array)\n","        return to_3_channels(channels[0], channels[1], channels[2])\n","    else:\n","        pixel_array = image.pixel_array\n","        pixel_array = hu_to_pixels(pixel_array, image.RescaleIntercept, image.RescaleSlope, window[0], window[1])\n","        pixel_array = segment_brain(pixel_array)\n","        pixel_array = apply_new_spacing(pixel_array, np.array(image.PixelSpacing), [1, 1])\n","        pixel_array = crop_or_reshape(pixel_array, dim)\n","        if apply_clahe:\n","            pixel_array = clahe(pixel_array)\n","        pixel_array = cv2.cvtColor(pixel_array.astype('uint8'), cv2.COLOR_GRAY2BGR)\n","        return pixel_array"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_neighbors_to_image(image_id, dataframe):\n","    \"\"\"\n","    Finds the neighbouring slices to an image. If there is no neighbours, an array filled with zeros will be returned.\n","        \n","    :param image_id: the image ID for which the neighbours will be found\n","    :param dataframe: the dataframe with image IDs, studies and vertical positions\n","        \n","    :return: (bottom neighbour, top neighbour)\n","    \"\"\"\n","    study_id = dataframe[dataframe['ID'] == image_id][\"Study\"].values[0]\n","    study_ids = dataframe[dataframe['Study'] == study_id][\"ID\"].unique()\n","    positions = []\n","    for img_id in study_ids:\n","        positions.append(dataframe[dataframe['ID'] == img_id][\"Position\"].values[0])\n","    positions_dict = dict(zip(positions, study_ids))\n","    sorted_keys = sorted(positions_dict)\n","    \n","    pos = dataframe[dataframe['ID'] == image_id][\"Position\"].values[0]\n","    pos_index = sorted_keys.index(pos)\n","    \n","    b_neigh_pos = sorted_keys[pos_index - 1] if pos_index != 0 else sorted_keys[pos_index]\n","    b_neigh = positions_dict[b_neigh_pos]\n","    t_neigh_pos = sorted_keys[pos_index + 1] if pos_index != len(sorted_keys) - 1 else sorted_keys[pos_index]\n","    t_neigh = positions_dict[t_neigh_pos]\n","    return b_neigh, t_neigh"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_images(dataframe, window, dim, output_dir, img_dir_path, context_3d=False, apply_clahe=False, limited_studies=None):\n","    \"\"\"\n","    Generates preprocessed .jpg images from .dicom CT scans.\n","        \n","    :param dataframe: a dataframe with image IDs, studies and its vertical positions\n","    :param window: a window to apply -> tuple in format (window center, wineow width)\n","    :param dim: a tuple with desired image dimensions\n","    :param output_dir: path to the directory for output files\n","    :param img_dir_path: path to the directory with .dicom files\n","    :param context_3d: whether to include 3D context or not\n","    :param apply_clahe: whether to apply CLAHE or not\n","    :param limited_studies: in the case of limited runtime, the current subset of studies to convert\n","    \"\"\"\n","    studies = limited_studies\n","    if studies is None:\n","        studies = dataframe[\"Study\"].unique()\n","\n","    for study in tqdm(studies):\n","        images = dataframe[dataframe[\"Study\"] == study][\"ID\"].unique()\n","\n","        for img_id in images:\n","            img = pydicom.filereader.dcmread(img_dir_path + img_id + \".dcm\")\n","            if not context_3d:\n","                img = img_preprocessing(img, dim=dim, window=window, apply_clahe=apply_clahe)\n","            else:\n","                b_neigh, t_neigh = find_neighbors_to_image(img_id, dataframe)\n","                b_neigh_img = pydicom.filereader.dcmread(img_dir_path + b_neigh + \".dcm\")\n","                t_neigh_img = pydicom.filereader.dcmread(img_dir_path + t_neigh + \".dcm\")\n","                img = img_preprocessing(img, b_neigh_img, t_neigh_img, dim, window, apply_clahe)\n","            cv2.imwrite(output_dir + img_id + \".jpg\" , img)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_IMG_DIR_PATH = \"/content/drive/MyDrive/rsna-intracranial-hemorrhage-detection/stage_2_train/\"\n","TEST_IMG_DIR_PATH = \"/content/drive/MyDrive/rsna-intracranial-hemorrhage-detection/stage_2_test/\"\n","OUTPUT_TRAIN_DIR = \"/content/jpg_data/stage_2_train_jpg/\"\n","OUTPUT_TEST_DIR = \"/content/jpg_data/stage_2_test_jpg/\"\n","TRAIN_DATAFRAME_PATH = \"/content/dataframes/stage_2_train_mod.csv\"\n","TEST_DATAFRAME_PATH = \"/content/dataframes/stage_2_sample_submission_mod.csv\"\n","\n","# use '! mkdir directory_name' if the directories didn't exist\n","\n","WINDOW = (40, 80)\n","IMG_DIM = (224, 224)\n","CONTEXT_3D = False\n","CLAHE = False\n","\n","######################################################################################################################\n","# in the case of limited runtime please modify the settings below\n","# converting all images may take hours\n","\n","LIMITED_RUNTIME = False   # set to True in the case of limited runtime\n","DATA_TO_CONVERT = \"train\"   # use \"train\" or \"test\", depending on which images you want to convert now\n","STUDIES_PER_RUN = 1000\n","RUNS_FINISHED = 0   # runs finished on current dataframe - if you start to convert test data, set again to 0\n","######################################################################################################################\n","\n","# generate training images\n","if not LIMITED_RUNTIME or DATA_TO_CONVERT == \"train\":\n","    print(\"Generating training images...\")\n","    dataframe = pd.read_csv(TRAIN_DATAFRAME_PATH)\n","    if not LIMITED_RUNTIME:\n","        generate_images(dataframe, WINDOW, IMG_DIM, OUTPUT_TRAIN_DIR, TRAIN_IMG_DIR_PATH, CONTEXT_3D, CLAHE)\n","    else:\n","        studies = dataframe['Study'].unique()\n","        if RUNS_FINISHED * STUDIES_PER_RUN > len(studies):\n","            print(f'No additional data to convert.')\n","        else:\n","            print(f'Run {RUNS_FINISHED + 1}. {int(np.ceil(len(studies)/STUDIES_PER_RUN)) - RUNS_FINISHED - 1} additional runs needed.')\n","            limited_studies = studies[RUNS_FINISHED * STUDIES_PER_RUN : (RUNS_FINISHED + 1) * STUDIES_PER_RUN]\n","            generate_images(dataframe, WINDOW, IMG_DIM, OUTPUT_TRAIN_DIR, TRAIN_IMG_DIR_PATH, CONTEXT_3D, CLAHE, limited_studies)\n","\n","# generate test images\n","if not LIMITED_RUNTIME or DATA_TO_CONVERT == \"test\":\n","    print(\"Generating test images...\")\n","    dataframe = pd.read_csv(TEST_DATAFRAME_PATH)\n","    if not LIMITED_RUNTIME:\n","        generate_images(dataframe, WINDOW, IMG_DIM, OUTPUT_TEST_DIR, TEST_IMG_DIR_PATH, CONTEXT_3D, CLAHE)\n","    else:\n","        studies = dataframe['Study'].unique()\n","        if RUNS_FINISHED * STUDIES_PER_RUN > len(studies):\n","            print(f'No additional data to convert.')\n","        else:\n","            print(f'Run {RUNS_FINISHED + 1}. {int(np.ceil(len(studies)/STUDIES_PER_RUN)) - RUNS_FINISHED - 1} additional runs needed.')\n","            limited_studies = studies[RUNS_FINISHED * STUDIES_PER_RUN : (RUNS_FINISHED + 1) * STUDIES_PER_RUN]\n","            generate_images(dataframe, WINDOW, IMG_DIM, OUTPUT_TEST_DIR, TEST_IMG_DIR_PATH, CONTEXT_3D, CLAHE, limited_studies)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# you can also tar and download the data as tar.gz\n","! tar -cf stage_2_train_jpg.tar.gz $OUTPUT_TRAIN_DIR'*'\n","! tar -cf stage_2_test_jpg.tar.gz $OUTPUT_test_DIR'*'"]}]}