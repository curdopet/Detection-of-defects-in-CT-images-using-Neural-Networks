{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras\nimport pandas as pd\nimport wandb\n\nfrom tqdm.notebook import tqdm\nfrom wandb.keras import WandbCallback\nfrom sklearn.model_selection import KFold\n\n%run /content/src/model_functions.ipynb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_metrics(y_test, predictions):\n    auc = tf.keras.metrics.AUC(multi_label=True)\n    accuracy = tf.keras.metrics.Accuracy()\n    tn = tf.keras.metrics.TrueNegatives()\n    tp = tf.keras.metrics.TruePositives()\n    fn = tf.keras.metrics.FalseNegatives()\n    fp = tf.keras.metrics.FalsePositives()\n\n    tn.update_state(y_test, predictions)\n    tp.update_state(y_test, predictions)\n    fn.update_state(y_test, predictions)\n    fp.update_state(y_test, predictions)\n    auc.update_state(y_test, predictions)\n    accuracy.update_state(y_test, np.around(predictions))\n    print(f'AUC: {auc.result().numpy()}, ' +\n          f'accuracy: {accuracy.result().numpy()}, ' +\n          f'sensitivity: {tp.result().numpy()/(tp.result().numpy()+fn.result().numpy())}, ' +\n          f'specificity: {tn.result().numpy()/(tn.result().numpy()+fp.result().numpy())}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the final model","metadata":{}},{"cell_type":"code","source":"RD_SEED = 123\nN_SPLITS = 3\nEPOCHS = 5\nTRAIN_DF_PATH = \"/content/dataframes/train_final.csv\"\nTRAIN_IMG_PATH = \"/content/drive/MyDrive/jpg_data/stage_2_train_jpg/\"\n\nARCHITECTURE = \"DenseNet121\" # or \"EfficientNet121\"\nBATCH_SIZE = 32\nDIM = (224, 224)\nN_CLASSES = 6\nWINDOW = (40, 80)\nIMAGE_FORMAT = 'jpg'\nLEARNING_RATE = 0.00001      # or 0.00002\nPOOLING = 'avg'\nMETRICS = [tf.keras.metrics.AUC(multi_label=True)]\nOPTIMIZER = tf.keras.optimizers.Adam\n\nLOAD_WEIGHTS_PATH = None     # path to weights you want to use for training \nSAVE_WEIGHTS = True\nOUTPUT_DIR = \"/content/weights/\"\n\n# in the case of limited runtime or the preference of running a specific fold:\nRUN_SINGLE_FOLD = False\nFOLD = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe = pd.read_csv(TRAIN_DF_PATH)\n\ndataframe.drop(index=dataframe.loc[dataframe['ID'] == \"ID_6431af929\"].index, inplace=True)\ndataframe.drop(index=dataframe.loc[dataframe['ID'] == \"ID_00de64f80\"].index, inplace=True)\nstudies = dataframe['Study'].unique()\n    \nfor i, (train, valid) in enumerate(KFold(n_splits=N_SPLITS, shuffle=True, random_state=RD_SEED).split(studies)):\n    print(f'Cross-validation fold {i+1}')\n    if RUN_SINGLE_FOLD and i+1 != FOLD:\n        print(\"Skipped...\")\n        continue\n        \n    tf.keras.backend.clear_session()\n    model = create_model_w(ARCHITECTURE, (224, 224, 3), POOLING, OPTIMIZER, LEARNING_RATE, weighted_multi_label_log_loss, METRICS, LOAD_WEIGHTS_PATH)\n    \n    for epoch in range(EPOCHS):\n        print(f'Epoch {epoch+1}')\n        \n        X_train, y_train, X_valid, y_valid = get_balanced_train_valid_tuples(dataframe, studies[train], studies[valid])\n        training_generator = DataGenerator(X_train, y_train, TRAIN_IMG_PATH, dataframe, BATCH_SIZE, DIM, N_CLASSES, shuffle=True, window=WINDOW, image_format=IMAGE_FORMAT)\n        validation_generator = DataGenerator(X_valid, y_valid, TRAIN_IMG_PATH, dataframe,BATCH_SIZE, DIM, N_CLASSES, shuffle=True, window=WINDOW, augment=False, image_format=IMAGE_FORMAT)\n\n        model.fit(x=training_generator, epochs=1)\n        if SAVE_WEIGHTS:\n            model.save_weights(OUTPUT_DIR + \"weights_\" + ARCHITECTURE + \"_fold_\" + str(i+1) + \"_epoch_\" + str(epoch+1) + \".h5\")\n        model.evaluate(x=validation_generator)\n    \n    if SAVE_WEIGHTS:\n        model.save_weights(OUTPUT_DIR + \"weights_\" + ARCHITECTURE + \"_fold_\" + str(i+1) + \".h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the model on custom test set","metadata":{}},{"cell_type":"code","source":"TEST_DF_PATH = \"/content/dataframes/test_final.csv\"\nTEST_IMG_PATH = \"/content/drive/MyDrive/jpg_data/stage_2_train_jpg/\"\n\nARCHITECTURE = \"DenseNet121\"   # or \"EfficientNet121\"\nBATCH_SIZE = 32\nLEARNING_RATE = 0.00001        # or 0.00002\nPOOLING = 'avg'\nMETRICS = [tf.keras.metrics.AUC(multi_label=True)]\nOPTIMIZER = tf.keras.optimizers.Adam\n\nWEIGHTS_DIR = \"/content/weights/\"\nWEIGHTS = \"weights_DenseNet121_fold_1.h5\"  # modify to use other weights\nUSE_TTA = True\nSAVE_PREDICTIONS = True\nOUTPUT_DIR = \"/content/predictions/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, batches, img_path, tta):\n    predictions = []\n    augmentation = DataAugmentation(123, max_angle=30)\n    \n    for batch in tqdm(batches):\n        samples = np.empty((len(batch), 224, 224, 3))\n        for i, ID in enumerate(batch):\n            samples[i] = cv2.imread(img_path + ID + \".jpg\") # TODO: add dicom possibility\n\n        if not tta:\n            predictions.extend(model.predict(samples))\n        else:\n            n_samples = 5\n            sample_predictions = []\n            for n in range(n_samples):\n                aug_samples = np.empty((len(batch), 224, 224, 3))\n                for s in range(len(batch)):\n                    aug_samples[s] = augmentation.random_augment(samples[s], False)\n                sample_predictions.append(model.predict(aug_samples))\n            sample_predictions = np.array(sample_predictions)\n            predictions.extend([[sample_predictions[:,y,x].sum()/n_samples for x in range(6)] for y in range(len(batch))])\n    return np.array(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe = pd.read_csv(TEST_DF_PATH)\n\nX_test, y_test = dataframe['ID'].values[::6], np.reshape(dataframe['Label'].values, (-1, 6))\nX_test_batches = np.array_split(X_test, len(X_test)//BATCH_SIZE)\n\ntf.keras.backend.clear_session()\nmodel = create_model_w(ARCHITECTURE, (224, 224, 3), POOLING, OPTIMIZER, LEARNING_RATE, weighted_multi_label_log_loss, METRICS, WEIGHTS_DIR + WEIGHTS)\n\npredictions = predict(model, X_test_batches, TEST_IMG_PATH, USE_TTA)\n\nif SAVE_PREDICTIONS:\n    pd.DataFrame(predictions).to_csv(OUTPUT_DIR + WEIGHTS[:-3] + (\"_TTA\" if USE_TTA else \"\") + \"_predictions.csv\", index=False)\n\nprint_metrics(y_test, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensembling the predictions","metadata":{}},{"cell_type":"code","source":"TEST_DF_PATH = \"/content/dataframes/test_final.csv\"\nPREDICTIONS_DIR = \"/content/predictions/\"\nPREDICTIONS_FILES = [\"pred1.csv\", \"pred2.csv\", \"pred3.csv\"]\nSAVE_PREDICTIONS = False\nOUTPUT_DIR = \"/content/predictions/\"\nOUTPUT_FILE = \"DenseNet121_ensemble_predictions.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(TEST_DF_PATH)\ny_test = np.reshape(test_df['Label'].values, (-1, 6))\n\npredictions = np.zeros_like(y_test).astype('float64')\nfor file in PREDICTIONS_FILES:\n    df = pd.read_csv(PREDICTIONS_DIR + file)\n    predictions += df.to_numpy()\npredictions /= len(PREDICTIONS_FILES)\n\nif SAVE_PREDICTIONS:\n    pd.DataFrame(predictions).to_csv(OUTPUT_DIR + OUTPUT_FILE, index=False)\n\nprint_metrics(y_test, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict the submission dataset","metadata":{}},{"cell_type":"code","source":"MOD_SUBMISSION_DF_PATH = \"/content/dataframes/stage_2_sample_submission_mod.csv\"\nSUBMISSION_IMG_PATH = \"/content/drive/MyDrive/jpg_data/stage_2_test_jpg/\"\nORIG_SUBMISSION_DF_PATH = \"/content/drive/MyDrive/rsna-intracranial-hemorrhage-detection/stage_2_sample_submission.csv\"\n\nARCHITECTURE = \"DenseNet121\"   # or \"EfficientNet121\"\nBATCH_SIZE = 32\nLEARNING_RATE = 0.00001        # or 0.00002\nPOOLING = 'avg'\nMETRICS = [tf.keras.metrics.AUC(multi_label=True)]\nOPTIMIZER = tf.keras.optimizers.Adam\n\nWEIGHTS_DIR = \"/content/weights/\"\nWEIGHTS = \"weights_DenseNet121_fold_1.h5\"\nUSE_TTA = True\nSAVE_PREDICTIONS = True\nOUTPUT_DIR = \"/content/predictions/\"\nOUTPUT_FILE = \"DenseNet121_fold_1_submission.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe = pd.read_csv(MOD_SUBMISSION_DF_PATH)\n\nX_test = dataframe['ID'].values[::6]\nX_test_batches = np.array_split(X_test, len(X_test)//BATCH_SIZE)\n\ntf.keras.backend.clear_session()\nmodel = create_model_w(ARCHITECTURE, (224, 224, 3), POOLING, OPTIMIZER, LEARNING_RATE, weighted_multi_label_log_loss, METRICS, WEIGHTS_DIR + WEIGHTS)\n\npredictions = predict(model, X_test_batches, SUBMISSION_IMG_PATH, USE_TTA)\n\nsubmission_df = pd.read_csv(ORIG_SUBMISSION_DF_PATH)\nsubmission_df['Label'] = predictions.flatten()\n\nif SAVE_PREDICTIONS:\n    submission_df.to_csv(OUTPUT_DIR + OUTPUT_FILE, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensembling submissions","metadata":{}},{"cell_type":"code","source":"ORIG_SUBMISSION_DF_PATH = \"/content/drive/MyDrive/rsna-intracranial-hemorrhage-detection/stage_2_sample_submission.csv\"\nPREDICTIONS_DIR = \"/content/predictions/\"\nSUBMISSION_FILES = [\"sub1.csv\", \"sub2.csv\", \"sub3.csv\", \"sub4.csv\", \"sub5.csv\"]\nSAVE_PREDICTIONS = True\nOUTPUT_DIR = \"/content/predictions/\"\nOUTPUT_FILE = \"EfficientDenseNet_ensemble_submission.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv(ORIG_SUBMISSION_DF_PATH)\ny_sub = sub_df['Label'].values\n\nsubmissions = np.zeros_like(y_sub).astype('float64')\nfor file in SUBMISSION_FILES:\n    df = pd.read_csv(PREDICTIONS_DIR + file)\n    submissions += df['Label'].values\nsubmissions /= len(SUBMISSION_FILES)\nsub_df['Label'] = submissions\n\nif SAVE_PREDICTIONS:\n    sub_df.to_csv(OUTPUT_DIR + OUTPUT_FILE, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}